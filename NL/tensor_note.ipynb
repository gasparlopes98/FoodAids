{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/golopes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"bidirectional_23\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m testing_lab_padded \u001b[39m=\u001b[39m prepare_seq(testing_labels,max_length_label)\n\u001b[1;32m     99\u001b[0m \u001b[39m# print(testing_sequences)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m# print(testing_lab_padded)\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mSequential([\n\u001b[1;32m    102\u001b[0m     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mEmbedding(vocab_size, embedding_dim, input_length\u001b[39m=\u001b[39;49mmax_length_sentence),\n\u001b[1;32m    103\u001b[0m     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mGlobalAveragePooling1D(),\n\u001b[1;32m    104\u001b[0m     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mDense(\u001b[39m32\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    105\u001b[0m     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mBidirectional(tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mLSTM(\u001b[39m32\u001b[39;49m)),\n\u001b[1;32m    106\u001b[0m     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mDense(max_length_label)\n\u001b[1;32m    107\u001b[0m     \u001b[39m# tf.keras.layers.Dense(24, activation='sigmoid')\u001b[39;49;00m\n\u001b[1;32m    108\u001b[0m ])\n\u001b[1;32m    110\u001b[0m \u001b[39m# tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m#     tf.keras.layers.GlobalAveragePooling1D(),\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m#     tf.keras.layers.Dense(24, activation='relu'),\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m#     tf.keras.layers.Dense(1, activation='sigmoid')\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[39m# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\u001b[39;00m\n\u001b[1;32m    116\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m    117\u001b[0m     loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m    118\u001b[0m     optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    119\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    120\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/work/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/work/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/work/lib/python3.10/site-packages/keras/engine/input_spec.py:232\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    230\u001b[0m     ndim \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mrank\n\u001b[1;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39m!=\u001b[39m spec\u001b[39m.\u001b[39mndim:\n\u001b[0;32m--> 232\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    233\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    234\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m, found ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mmax_ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"bidirectional_23\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 32)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') # install NLTK data to home user directory\n",
    "from nltk.corpus import stopwords\n",
    "import unidecode\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^a-z 0-9 #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STOPWORDS_RE = re.compile(r\"\\b(\" + \"|\".join(STOPWORDS) + \")\\\\W\")\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    \n",
    "    #text = # lowercase text\n",
    "    text = unidecode.unidecode(text).lower()\n",
    "    #text = # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(\" \", text)\n",
    "    #text = # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = BAD_SYMBOLS_RE.sub(\" \", text)\n",
    "    # remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "\n",
    "    #text = # delete stopwords from text\n",
    "    text = STOPWORDS_RE.sub(\"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n",
    "  \n",
    "def prepare_seq(sequence,max):\n",
    "    sequence = tokenizer.texts_to_sequences(sequence)\n",
    "    sequence = pad_sequences(sequence, maxlen=max, padding=padding_type, truncating=trunc_type)\n",
    "    return  np.array(sequence)\n",
    "\n",
    "rows = []\n",
    "dataset = []\n",
    "with open('../recipes.csv') as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    header = next(csvreader)\n",
    "    for row in csvreader:\n",
    "        rows.append(row)\n",
    "\n",
    "random.shuffle(rows)\n",
    "\n",
    "training_size =60\n",
    "vocab_size = 1000\n",
    "embedding_dim = 32\n",
    "num_epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "labels=[]\n",
    "sentences=[]\n",
    "max_length_sentence=100\n",
    "max_length_label=20\n",
    "\n",
    "for row in rows:\n",
    "    recipes=row[4]\n",
    "    label=' '.join(row[:2])\n",
    "    sentences.append(text_prepare(recipes))\n",
    "    labels.append(label)\n",
    "\n",
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]\n",
    "    \n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences=prepare_seq(training_sentences,max_length_sentence)\n",
    "testing_sequences = prepare_seq(testing_sentences,max_length_sentence)\n",
    "training_lab_padded = prepare_seq(training_labels,max_length_label)\n",
    "testing_lab_padded = prepare_seq(testing_labels,max_length_label)\n",
    "# print(testing_sequences)\n",
    "# print(testing_lab_padded)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length_sentence),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(max_length_label)\n",
    "    # tf.keras.layers.Dense(24, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "#     tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#     tf.keras.layers.Dense(24, activation='relu'),\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(training_sequences, training_lab_padded, batch_size=batch_size, epochs=num_epochs, validation_data=(testing_sequences, testing_lab_padded), verbose=2)\n",
    "\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olive,bay,potatoes,onion,saute,mixture,beef,oil\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_txt = open(\"../test.txt\", \"r\")\n",
    "input_padded=prepare_seq(input_txt,max_length_sentence)\n",
    "input_padded = np.squeeze(input_padded)\n",
    "\n",
    "result = model.predict_on_batch(tf.expand_dims(input_padded,0))\n",
    "\n",
    "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
    "\n",
    "result_final=[]\n",
    "for a in np.squeeze(result):\n",
    "    for i in reverse_word_index:\n",
    "        if(int(a)==i):\n",
    "            if(reverse_word_index[i] not in result_final):\n",
    "                result_final.append(reverse_word_index[i])\n",
    "            break\n",
    "\n",
    "print(','.join(result_final))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
